{"cells":[{"cell_type":"markdown","metadata":{"id":"GOp0ON-xFniU"},"source":["## Reference\n","\n","http://deepyeti.ucsd.edu/jianmo/amazon/index.html"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7787,"status":"ok","timestamp":1617211906137,"user":{"displayName":"James Tang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwTEr6KMZe9m9QXoqfZGIyRH712EN_M7q39JNEDoA=s64","userId":"16529627775516976202"},"user_tz":-480},"id":"YqjigRX4bDNo"},"outputs":[],"source":["!pip install -q tensorflow-text\n","!pip install -q tf-models-official"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WOi7CX54bbo4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import shutil\n","import random\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optmizer\n","import glob\n","import json \n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import nltk\n","from nltk.corpus import stopwords\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","import string\n","import gzip\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1HJhhLg7bsQ2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_Tnltdgrf7JF"},"outputs":[{"name":"stdout","output_type":"stream","text":["BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"]}],"source":["#@title Choose a BERT model to fine-tune\n","\n","bert_model_name = 'small_bert/bert_en_uncased_L-4_H-768_A-12'  #@param [\"bert_en_uncased_L-24_H-1024_A-16\" , \"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-24_H-1024_A-16':\n","      'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'electra_small':\n","        'https://tfhub.dev/google/electra_small/2',\n","    'electra_base':\n","        'https://tfhub.dev/google/electra_base/2',\n","    'experts_pubmed':\n","        'https://tfhub.dev/google/experts/bert/pubmed/2',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-24_H-1024_A-16':\n","      'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n","    'electra_small':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'electra_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_pubmed':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3m23rrNHjvUX"},"outputs":[],"source":["bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","bert_model = hub.KerasLayer(tfhub_handle_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SV_7lwypkMJg"},"outputs":[{"name":"stdout","output_type":"stream","text":["Keys       : ['input_mask', 'input_type_ids', 'input_word_ids']\n","Shape      : (1, 128)\n","Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n","Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n","Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n","Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1\n","Pooled Outputs Shape:(1, 768)\n","Pooled Outputs Values:[ 0.99482054  0.38433293  0.21771398  0.9999552   0.33607924 -0.41822514\n","  0.09995244 -0.7727754  -0.03063799 -0.6972299  -0.48701775 -0.99855006]\n","Sequence Outputs Shape:(1, 128, 768)\n","Sequence Outputs Values:[[ 0.660055   -0.0349792   0.12217315 ... -0.36853275 -0.02229888\n","   0.0520366 ]\n"," [-0.52050245  0.38648155  0.21641293 ...  0.6125513  -0.50356567\n","   0.06208777]\n"," [-0.05385322  0.362691   -0.7842814  ... -0.10821293 -0.11559598\n","  -0.34593078]\n"," ...\n"," [ 0.38902903  0.03228627  0.00352081 ...  0.17780164 -0.20709127\n","   0.1807682 ]\n"," [ 0.41474122 -0.00388229  0.09981684 ...  0.25926298 -0.12712786\n","   0.20766746]\n"," [ 0.28740555 -0.01674991  0.14565508 ...  0.34437442 -0.17614466\n","   0.19079393]]\n"]}],"source":["text_test = ['this is such an amazing movie!']\n","text_preprocessed = bert_preprocess_model(text_test)\n","\n","print(f'Keys       : {list(text_preprocessed.keys())}')\n","print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n","print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n","print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n","print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n","\n","bert_results = bert_model(text_preprocessed)\n","\n","print(f'Loaded BERT: {tfhub_handle_encoder}')\n","print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n","print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n","print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n","print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"]},{"cell_type":"markdown","metadata":{"id":"U_F0PjeCqKBw"},"source":["## Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AVvyd4HpkZWm"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to \u003cfunction recreate_function.\u003clocals\u003e.restored_function_body at 0x7f91a5d3bc20\u003e triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to \u003cfunction recreate_function.\u003clocals\u003e.restored_function_body at 0x7f91a5d3bc20\u003e triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:6 out of the last 6 calls to \u003cfunction recreate_function.\u003clocals\u003e.restored_function_body at 0x7f91a5d4e320\u003e triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:6 out of the last 6 calls to \u003cfunction recreate_function.\u003clocals\u003e.restored_function_body at 0x7f91a5d4e320\u003e triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]}],"source":["def build_classifier_model():\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(2048, activation=\"relu\")(net)\n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(512, activation=\"relu\")(net)\n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(6, activation='softmax', name='classifier')(net)\n","  return tf.keras.Model(text_input, net)\n","\n","SentimentModel = build_classifier_model()\n","tf.keras.utils.plot_model(SentimentModel)\n","\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","metrics = tf.keras.metrics.CategoricalAccuracy(\n","    name='categorical_accuracy', \n","    dtype=None\n",")"]},{"cell_type":"markdown","metadata":{"id":"7K7s2LJEqGgn"},"source":["## Working with data"]},{"cell_type":"markdown","metadata":{"id":"kSmILyJhqgc1"},"source":["### Define data location"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1mPCbxX0qknG"},"outputs":[],"source":["COMPRESSED_ROOT = '/content/drive/MyDrive/NUS/CS5260/projects/data/All_Amazon_Review.json.gz'\n","DATA_ROOT = '/content/drive/MyDrive/NUS/CS5260/projects/data/Electronics.json.gz'\n","SAMPLE_ROOT = '/content/drive/MyDrive/NUS/CS5260/projects/sample_data/Magazine_Subscriptions.json.gz'"]},{"cell_type":"markdown","metadata":{"id":"aTwDCscRtWYV"},"source":["### Unpacking data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gNAABN9StVyL"},"outputs":[],"source":["# import gzip\n","# f=gzip.open(COMPRESSED_ROOT,'rb')\n","# file_content=f.read()\n","# print(file_content)"]},{"cell_type":"markdown","metadata":{"id":"GL1W-uKQth6X"},"source":["### Max String"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"awoQcsm7qWxD"},"outputs":[],"source":["def find_max_string():   \n","    # the goal of this function is to find the max token among all the json files in directory\n","    MAX_YET = 0\n","    MAX_STRING = ''\n","    IDX = 0\n","    FILES =  glob.glob(DATA_ROOT)\n","    for file in tqdm(FILES):\n","        print('Processing : {0}'.format(file))\n","        data = pd.read_json(file, lines=True)\n","        for idx, d in enumerate(data['reviewText']):\n","            if len(str(d)) \u003e MAX_YET:\n","                MAX_YET = len(str(d))\n","                IDX = idx\n","        print('String max: {0}'.format(MAX_YET))\n","        MAX_STRING = data['reviewText'][IDX]\n","\n","    print('Global MAX: {0}'.format(MAX_YET))\n","    print('Global MAX_STRING: {0}'.format(MAX_STRING))"]},{"cell_type":"markdown","metadata":{"id":"6mnumk3WD03o"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"vntHtFPntssx"},"source":["### PoC on a small subset of data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K8vwqWw3tyxi"},"outputs":[],"source":["def parse(path):\n","  g = gzip.open(path, 'rb')\n","  for l in g:\n","    yield json.loads(l)\n","\n","def getDF(path):\n","  i = 0\n","  df = {}\n","  for d in parse(path):\n","    df[i] = d\n","    i += 1\n","  return pd.DataFrame.from_dict(df, orient='index')\n","\n","def prepareData(fp):\n","  data = getDF(fp)\n","  train_text = data['reviewText'].to_numpy(dtype='str')\n","  train_label = data['overall'].to_numpy()\n","  train_label = train_label.reshape((len(train_label), 1))\n","  train_label = tf.keras.utils.to_categorical(train_label, num_classes=6)\n","  train_label = np.asarray(train_label).astype(np.int)\n","  X_train, X_test, y_train, y_test = train_test_split(train_text, train_label, test_size=0.2, shuffle=True)\n","  return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = prepareData(SAMPLE_ROOT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clfxFrFNlMHy"},"outputs":[],"source":["print(X_train[:1])\n","print(X_train.shape)\n","print(y_train[:1])\n","print(y_train.shape)\n","\n","# X_train = X_train[:10]\n","# y_train = y_train[:10]\n","\n","MODEL_NAME = \"k1\"\n","epochs = 5\n","batch_size = 128\n","steps_per_epoch = X_train.size\n","init_lr = 5e-5\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","print('epochs: {0}'.format(epochs))\n","print('init_lr: {0}'.format(init_lr))\n","print('batch_size: {0}'.format(batch_size))\n","\n","\n","# use optimizer = optimization.create_optimizer()\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')\n","\n","\n","print(SentimentModel.summary())\n","tf.keras.utils.plot_model(SentimentModel, to_file='./model/'+MODEL_NAME+'.png')\n","\n","checkpoint_filepath=\"./model/{epoch:02d}-{val_categorical_accuracy:.2f}/\"\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=False,\n","    monitor='val_categorical_accuracy',\n","    mode='max',\n","    verbose=1,\n","    save_best_only=False\n",")\n","\n","SentimentModel.compile(\n","    optimizer=optimizer,\n","    loss=loss,\n","    metrics=metrics\n",")\n","\n","print(f'Training model with {tfhub_handle_encoder}')\n","\n","history = SentimentModel.fit(\n","    x=X_train,\n","    y=y_train,\n","    epochs=epochs,\n","    validation_split = 0.1,\n","    batch_size = batch_size,\n","    callbacks=[model_checkpoint_callback]\n",")\n","\n","print(SentimentModel.metrics_names)\n"]},{"cell_type":"markdown","metadata":{"id":"knOvgMOSDx1B"},"source":["## Evaluation on test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbXFkRWy6OOk"},"outputs":[],"source":["loss, accuracy = SentimentModel.evaluate(x=X_test, y=y_test)\n","\n","print(f'Loss: {loss}')\n","print(f'Accuracy: {accuracy}')"]},{"cell_type":"markdown","metadata":{"id":"2_W-949SEBjw"},"source":["## Sanity Check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhMRGDmfDRKs"},"outputs":[],"source":["prediction = SentimentModel.predict(\n","    [\"\"\"irthday, and they are horrible. You can't fully scI received 3 $10 gift cards for my bratch off the card to get the claim code and when you do, you can't read it. I've tried and tried to enter the codes to buy some books with my cards. Invalid entry. They're useless. Don't get the cards.\"\"\"]\n",")\n","\n","print('predicted class: {0}'.format(np.argmax(prediction)))"]},{"cell_type":"markdown","metadata":{"id":"mVRBO-9uEHsw"},"source":["## Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzpS24NX6fHY"},"outputs":[],"source":["history_dict = history.history\n","print(history_dict.keys())\n","\n","acc = history_dict['categorical_accuracy']\n","val_acc = history_dict['val_categorical_accuracy']\n","g_loss = history_dict['loss']\n","val_loss = history_dict['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","fig = plt.figure(figsize=(10, 6))\n","fig.tight_layout()\n","\n","plt.subplot(2, 1, 1)\n","# \"bo\" is for \"blue dot\"\n","plt.plot(epochs, g_loss, 'r', label='Training loss')\n","# b is for \"solid blue line\"\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","# plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(epochs, acc, 'r', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')"]},{"cell_type":"markdown","metadata":{"id":"vwoR1CQ0G-uK"},"source":["## Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5T9tMo5G987"},"outputs":[],"source":["dataset_name = 'amazon'\n","saved_model_path = './model/{}_bert'.format(dataset_name.replace('/', '_'))\n","print(saved_model_path)\n","SentimentModel.save(saved_model_path, include_optimizer=False)"]},{"cell_type":"markdown","metadata":{"id":"nVCpqV7sRoSk"},"source":["## Reload Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1A3B_cclRnVq"},"outputs":[],"source":["reloaded_model = tf.keras.models.load_model('/content/model/amazon_bert')\n","\n","prediction = reloaded_model.predict(\n","    [\"\"\"irthday, and they are horrible. You can't fully scI received 3 $10 gift cards for my bratch off the card to get the claim code and when you do, you can't read it. I've tried and tried to enter the codes to buy some books with my cards. Invalid entry. They're useless. Don't get the cards.\"\"\"]\n",")\n","\n","print('predicted class: {0}'.format(np.argmax(prediction)))\n"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyNNdxzSF5llHzQl/iQjsL21","machine_shape":"hm","mount_file_id":"1Fp_RkvJBUY2yxVbnTZUJFboNu1_T0DVM","name":"sentimentAnalysis","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}